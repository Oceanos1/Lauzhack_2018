{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import initializers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "def init_weights(shape, name=None):\n",
    "    return initializers.normal(shape, scale=0.01, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "API_ENDPOINT_URL =  'http://lauzhack.sqpub.ch/histo_prices'\n",
    "r = requests.get(API_ENDPOINT_URL)\n",
    "raw_data = r.text.split('\\n')\n",
    "data_series = pd.Series(raw_data)\n",
    "data_series = data_series.str.split(' ')\n",
    "data = pd.DataFrame(data_series.values.tolist(), index=data_series.index)\n",
    "data.columns = ['timestamp', 'price']\n",
    "data['timestamp'] = pd.to_datetime(data.timestamp)\n",
    "time_stamps = data['timestamp']\n",
    "data.set_index('timestamp', inplace=True)\n",
    "data['price'] = data['price'].astype(float)\n",
    "mean_minute = data.groupby(pd.Grouper(freq='60s')).mean()[2881:].dropna()\n",
    "mean_minute\n",
    "\n",
    "\n",
    "input_step_size = 50\n",
    "output_size = 30\n",
    "sliding_window = False\n",
    "\n",
    "prices= data.loc[:,'price'].values\n",
    "times = data.loc[:,'price'].values\n",
    "prices.shape\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "outputs = []\n",
    "inputs = []\n",
    "output_times = []\n",
    "input_times = []\n",
    "if sliding_window:\n",
    "    for i in range(len(prices)-input_step_size-output_size):\n",
    "        inputs.append(prices[i:i + input_step_size])\n",
    "        input_times.append(times[i:i + input_step_size])\n",
    "        outputs.append(prices[i + input_step_size: i + input_step_size+ output_size])\n",
    "        output_times.append(times[i + input_step_size: i + input_step_size+ output_size])\n",
    "else:\n",
    "    for i in range(0,len(prices)-input_step_size-output_size, input_step_size):\n",
    "        inputs.append(prices[i:i + input_step_size])\n",
    "        input_times.append(times[i:i + input_step_size])\n",
    "        outputs.append(prices[i + input_step_size: i + input_step_size+ output_size])\n",
    "        output_times.append(times[i + input_step_size: i + input_step_size+ output_size])\n",
    "inputs= np.array(inputs)\n",
    "outputs= np.array(outputs)\n",
    "output_times = np.array(output_times)\n",
    "input_times = np.array(input_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "class PastSampler:\n",
    "    '''\n",
    "    Forms training samples for predicting future values from past value\n",
    "    '''\n",
    "     \n",
    "    def __init__(self, N, K, sliding_window = True):\n",
    "        '''\n",
    "        Predict K future sample using N previous samples\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.sliding_window = sliding_window\n",
    " \n",
    "    def transform(self, A):\n",
    "        M = self.N + self.K     #Number of samples per row (sample + target)\n",
    "        #indexes\n",
    "        if self.sliding_window:\n",
    "            I = np.arange(M) + np.arange(A.shape[0] - M + 1).reshape(-1, 1)\n",
    "        else:\n",
    "            if A.shape[0]%M == 0:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0],M).reshape(-1,1)\n",
    "                \n",
    "            else:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0] -M,M).reshape(-1,1)\n",
    "            \n",
    "        B = A[I].reshape(-1, M * A.shape[1], A.shape[2])\n",
    "        ci = self.N * A.shape[1]    #Number of features per sample\n",
    "        return B[:, :ci], B[:, ci:] #Sample matrix, Target matrix\n",
    "\n",
    "\n",
    "\n",
    "#Columns of price data to use\n",
    "columns = ['price']\n",
    "data = data.loc[:,columns]\n",
    "original_data = data.loc[:,columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# normalization\n",
    "for c in columns:\n",
    "    data[c] = scaler.fit_transform(data[c].values.reshape(-1,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "#Features are input sample dimensions(channels)\n",
    "A = np.array(data)[:,None,:]\n",
    "original_A = np.array(original_data)[:,None,:]\n",
    "time_stamps = np.array(time_stamps)[:,None,None]\n",
    "\n",
    "#Make samples of temporal sequences of pricing data (channel)\n",
    "NPS, NFS = 256, 16         #Number of past and future samples\n",
    "ps = PastSampler(NPS, NFS, sliding_window=False)\n",
    "B, Y = ps.transform(A)\n",
    "input_times, output_times = ps.transform(time_stamps)\n",
    "original_B, original_Y = ps.transform(original_A)\n",
    "\n",
    "file_name='bitcoin2015to2017_close.h5'\n",
    "\n",
    "import h5py\n",
    "with h5py.File(file_name, 'w') as f:\n",
    "    f.create_dataset(\"inputs\", data = B)\n",
    "    f.create_dataset('outputs', data = Y)\n",
    "    f.create_dataset(\"input_times\", data = np.string_(input_times))\n",
    "    f.create_dataset('output_times', data = np.string_(output_times))\n",
    "    f.create_dataset(\"original_datas\", data=np.array(original_data))\n",
    "    f.create_dataset('original_inputs',data=original_B)\n",
    "    f.create_dataset('original_outputs',data=original_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22 samples, validate on 6 samples\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.1479 - val_loss: 0.0035\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 329us/step - loss: 0.1061 - val_loss: 0.0021\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 369us/step - loss: 0.0692 - val_loss: 0.0015\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 491us/step - loss: 0.0749 - val_loss: 0.0014\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 509us/step - loss: 0.0461 - val_loss: 0.0015\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 858us/step - loss: 0.0423 - val_loss: 0.0017\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 550us/step - loss: 0.0431 - val_loss: 0.0017\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 502us/step - loss: 0.0387 - val_loss: 0.0016\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 522us/step - loss: 0.0346 - val_loss: 0.0015\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 587us/step - loss: 0.0309 - val_loss: 0.0014\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 703us/step - loss: 0.0286 - val_loss: 0.0014\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 651us/step - loss: 0.0266 - val_loss: 0.0014\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 517us/step - loss: 0.0294 - val_loss: 0.0015\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 618us/step - loss: 0.0207 - val_loss: 0.0016\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 603us/step - loss: 0.0234 - val_loss: 0.0017\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 668us/step - loss: 0.0163 - val_loss: 0.0016\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 506us/step - loss: 0.0160 - val_loss: 0.0015\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 457us/step - loss: 0.0186 - val_loss: 0.0015\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 706us/step - loss: 0.0152 - val_loss: 0.0015\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 777us/step - loss: 0.0223 - val_loss: 0.0017\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 861us/step - loss: 0.0155 - val_loss: 0.0023\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 687us/step - loss: 0.0186 - val_loss: 0.0019\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 785us/step - loss: 0.0177 - val_loss: 0.0013\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 348us/step - loss: 0.0171 - val_loss: 0.0012\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 309us/step - loss: 0.0144 - val_loss: 0.0013\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 425us/step - loss: 0.0164 - val_loss: 0.0018\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 475us/step - loss: 0.0139 - val_loss: 0.0018\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 621us/step - loss: 0.0140 - val_loss: 0.0016\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 421us/step - loss: 0.0123 - val_loss: 0.0014\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 743us/step - loss: 0.0139 - val_loss: 0.0014\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 768us/step - loss: 0.0122 - val_loss: 0.0016\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 604us/step - loss: 0.0111 - val_loss: 0.0017\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 620us/step - loss: 0.0143 - val_loss: 0.0015\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.0124 - val_loss: 0.0012\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 812us/step - loss: 0.0148 - val_loss: 0.0013\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 865us/step - loss: 0.0150 - val_loss: 0.0017\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 326us/step - loss: 0.0118 - val_loss: 0.0023\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 425us/step - loss: 0.0110 - val_loss: 0.0018\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 614us/step - loss: 0.0119 - val_loss: 0.0013\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 391us/step - loss: 0.0102 - val_loss: 0.0013\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0017\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 745us/step - loss: 0.0115 - val_loss: 0.0018\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 467us/step - loss: 0.0094 - val_loss: 0.0015\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 410us/step - loss: 0.0100 - val_loss: 0.0012\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 397us/step - loss: 0.0102 - val_loss: 0.0013\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 423us/step - loss: 0.0129 - val_loss: 0.0019\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 367us/step - loss: 0.0096 - val_loss: 0.0025\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 466us/step - loss: 0.0115 - val_loss: 0.0016\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 388us/step - loss: 0.0114 - val_loss: 0.0011\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.0111 - val_loss: 0.0011\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 714us/step - loss: 0.0098 - val_loss: 0.0017\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 708us/step - loss: 0.0095 - val_loss: 0.0022\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 449us/step - loss: 0.0101 - val_loss: 0.0020\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 523us/step - loss: 0.0088 - val_loss: 0.0013\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 768us/step - loss: 0.0108 - val_loss: 0.0011\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 405us/step - loss: 0.0108 - val_loss: 0.0018\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 598us/step - loss: 0.0077 - val_loss: 0.0022\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 492us/step - loss: 0.0106 - val_loss: 0.0017\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 499us/step - loss: 0.0096 - val_loss: 0.0012\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 532us/step - loss: 0.0137 - val_loss: 0.0016\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 464us/step - loss: 0.0083 - val_loss: 0.0021\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 571us/step - loss: 0.0111 - val_loss: 0.0019\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 498us/step - loss: 0.0074 - val_loss: 0.0016\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 406us/step - loss: 0.0089 - val_loss: 0.0016\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 387us/step - loss: 0.0102 - val_loss: 0.0015\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 752us/step - loss: 0.0085 - val_loss: 0.0019\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 483us/step - loss: 0.0103 - val_loss: 0.0022\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 467us/step - loss: 0.0085 - val_loss: 0.0020\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 350us/step - loss: 0.0092 - val_loss: 0.0017\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 363us/step - loss: 0.0094 - val_loss: 0.0017\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 343us/step - loss: 0.0087 - val_loss: 0.0015\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 399us/step - loss: 0.0087 - val_loss: 0.0015\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 510us/step - loss: 0.0067 - val_loss: 0.0016\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 582us/step - loss: 0.0077 - val_loss: 0.0017\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 328us/step - loss: 0.0080 - val_loss: 0.0018\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 486us/step - loss: 0.0078 - val_loss: 0.0018\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 495us/step - loss: 0.0083 - val_loss: 0.0015\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 501us/step - loss: 0.0076 - val_loss: 0.0013\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 588us/step - loss: 0.0107 - val_loss: 0.0015\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 451us/step - loss: 0.0074 - val_loss: 0.0017\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 624us/step - loss: 0.0079 - val_loss: 0.0015\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 513us/step - loss: 0.0089 - val_loss: 0.0016\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 627us/step - loss: 0.0070 - val_loss: 0.0016\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 481us/step - loss: 0.0074 - val_loss: 0.0018\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 476us/step - loss: 0.0065 - val_loss: 0.0021\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 564us/step - loss: 0.0083 - val_loss: 0.0022\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 480us/step - loss: 0.0090 - val_loss: 0.0020\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 425us/step - loss: 0.0077 - val_loss: 0.0017\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 399us/step - loss: 0.0071 - val_loss: 0.0018\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 523us/step - loss: 0.0062 - val_loss: 0.0019\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 631us/step - loss: 0.0063 - val_loss: 0.0016\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 583us/step - loss: 0.0075 - val_loss: 0.0014\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 464us/step - loss: 0.0071 - val_loss: 0.0013\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 638us/step - loss: 0.0090 - val_loss: 0.0017\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 529us/step - loss: 0.0059 - val_loss: 0.0025\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 354us/step - loss: 0.0069 - val_loss: 0.0025\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 714us/step - loss: 0.0078 - val_loss: 0.0023\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 385us/step - loss: 0.0060 - val_loss: 0.0020\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 542us/step - loss: 0.0075 - val_loss: 0.0018\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 403us/step - loss: 0.0073 - val_loss: 0.0020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd81fbd92e8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, LeakyReLU, PReLU\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import h5py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:\n",
    "    datas = hf['inputs'].value\n",
    "    labels = hf['outputs'].value\n",
    "\n",
    "\n",
    "output_file_name='bitcoin2015to2017_close_CNN_2_relu'\n",
    "\n",
    "step_size = datas.shape[1]\n",
    "batch_size= 8\n",
    "nb_features = datas.shape[2]\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "#split training validation\n",
    "training_size = int(0.8* datas.shape[0])\n",
    "training_datas = datas[:training_size,:]\n",
    "training_labels = labels[:training_size,:]\n",
    "validation_datas = datas[training_size:,:]\n",
    "validation_labels = labels[training_size:,:]\n",
    "#build model\n",
    "\n",
    "# 2 layers\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D( strides=4, filters=nb_features, kernel_size=16))\n",
    "\n",
    "'''\n",
    "# 3 Layers\n",
    "model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=8))\n",
    "#model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=8))\n",
    "#model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D( strides=2, filters=nb_features, kernel_size=8))\n",
    "# 4 layers\n",
    "model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=2, filters=8, kernel_size=2))\n",
    "#model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=2))\n",
    "#model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=2))\n",
    "#model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D( strides=2, filters=nb_features, kernel_size=2))\n",
    "'''\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(training_datas, training_labels,verbose=1, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.2333922 ],\n",
       "        [0.23536345],\n",
       "        [0.23784117],\n",
       "        [0.2378177 ],\n",
       "        [0.24119873],\n",
       "        [0.24708173],\n",
       "        [0.24626613],\n",
       "        [0.24562036],\n",
       "        [0.24333675],\n",
       "        [0.23850518],\n",
       "        [0.23936404],\n",
       "        [0.23516054],\n",
       "        [0.22236314],\n",
       "        [0.2201446 ],\n",
       "        [0.2086715 ],\n",
       "        [0.19840674]],\n",
       "\n",
       "       [[0.17188847],\n",
       "        [0.15913966],\n",
       "        [0.14431202],\n",
       "        [0.13650128],\n",
       "        [0.1310913 ],\n",
       "        [0.13013579],\n",
       "        [0.13477358],\n",
       "        [0.12859496],\n",
       "        [0.13095059],\n",
       "        [0.12634866],\n",
       "        [0.13155825],\n",
       "        [0.1366965 ],\n",
       "        [0.13634083],\n",
       "        [0.1419087 ],\n",
       "        [0.14959326],\n",
       "        [0.14830473]],\n",
       "\n",
       "       [[0.11832517],\n",
       "        [0.11316681],\n",
       "        [0.12172216],\n",
       "        [0.12848933],\n",
       "        [0.13104074],\n",
       "        [0.14549646],\n",
       "        [0.13797167],\n",
       "        [0.14005187],\n",
       "        [0.13876306],\n",
       "        [0.14434874],\n",
       "        [0.14887872],\n",
       "        [0.15233627],\n",
       "        [0.15859784],\n",
       "        [0.16052163],\n",
       "        [0.15967667],\n",
       "        [0.15438843]],\n",
       "\n",
       "       [[0.19398865],\n",
       "        [0.19439441],\n",
       "        [0.19510779],\n",
       "        [0.19229665],\n",
       "        [0.18927011],\n",
       "        [0.1891292 ],\n",
       "        [0.18446842],\n",
       "        [0.18008424],\n",
       "        [0.17120628],\n",
       "        [0.16996302],\n",
       "        [0.17045128],\n",
       "        [0.17046082],\n",
       "        [0.17069352],\n",
       "        [0.16761136],\n",
       "        [0.16489583],\n",
       "        [0.16080058]],\n",
       "\n",
       "       [[0.15925631],\n",
       "        [0.16466151],\n",
       "        [0.16839223],\n",
       "        [0.16657118],\n",
       "        [0.16893649],\n",
       "        [0.17317852],\n",
       "        [0.16780919],\n",
       "        [0.16963215],\n",
       "        [0.16458759],\n",
       "        [0.16333356],\n",
       "        [0.15920639],\n",
       "        [0.15699011],\n",
       "        [0.15112747],\n",
       "        [0.14648321],\n",
       "        [0.14328697],\n",
       "        [0.13852507]],\n",
       "\n",
       "       [[0.14614168],\n",
       "        [0.14579715],\n",
       "        [0.14710696],\n",
       "        [0.14703074],\n",
       "        [0.14940616],\n",
       "        [0.14936107],\n",
       "        [0.14825088],\n",
       "        [0.1450434 ],\n",
       "        [0.14386904],\n",
       "        [0.14197624],\n",
       "        [0.14228772],\n",
       "        [0.14543307],\n",
       "        [0.14802805],\n",
       "        [0.15263474],\n",
       "        [0.1546509 ],\n",
       "        [0.15620059]]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(validation_datas)\n",
    "predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22 samples, validate on 6 samples\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.1935 - val_loss: 0.0157\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.1839 - val_loss: 0.0132\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.1681 - val_loss: 0.0109\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.1573 - val_loss: 0.0086\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.1364 - val_loss: 0.0067\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.1167 - val_loss: 0.0057\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.1271 - val_loss: 0.0057\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.1415 - val_loss: 0.0050\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.1190 - val_loss: 0.0040\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1027 - val_loss: 0.0034\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0953 - val_loss: 0.0030\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1076 - val_loss: 0.0027\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0903 - val_loss: 0.0023\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.0842 - val_loss: 0.0020\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0974 - val_loss: 0.0017\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0928 - val_loss: 0.0015\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.0912 - val_loss: 0.0014\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.0772 - val_loss: 0.0012\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0873 - val_loss: 0.0011\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0867 - val_loss: 9.6947e-04\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0791 - val_loss: 8.6299e-04\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0720 - val_loss: 7.6539e-04\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0809 - val_loss: 8.7124e-04\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0745 - val_loss: 8.4658e-04\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0768 - val_loss: 7.6185e-04\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.0582 - val_loss: 6.5223e-04\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0605 - val_loss: 6.3505e-04\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0651 - val_loss: 7.3614e-04\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0718 - val_loss: 6.1387e-04\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0641 - val_loss: 4.1734e-04\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0729 - val_loss: 3.6666e-04\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0617 - val_loss: 3.8040e-04\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0703 - val_loss: 3.7807e-04\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0596 - val_loss: 4.4824e-04\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0651 - val_loss: 5.2375e-04\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0577 - val_loss: 4.1730e-04\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0608 - val_loss: 4.3263e-04\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0507 - val_loss: 6.6917e-04\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.0495 - val_loss: 0.0012\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0570 - val_loss: 9.1551e-04\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0585 - val_loss: 6.9118e-04\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0504 - val_loss: 6.5591e-04\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0401 - val_loss: 6.3221e-04\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0703 - val_loss: 5.1503e-04\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0547 - val_loss: 4.9678e-04\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0571 - val_loss: 5.1582e-04\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0447 - val_loss: 6.0277e-04\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0409 - val_loss: 7.7527e-04\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0341 - val_loss: 8.2974e-04\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0504 - val_loss: 8.4588e-04\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0343 - val_loss: 8.5592e-04\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0400 - val_loss: 7.6061e-04\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0487 - val_loss: 6.9549e-04\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0408 - val_loss: 7.6070e-04\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0351 - val_loss: 8.0699e-04\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0473 - val_loss: 6.4804e-04\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0535 - val_loss: 6.8886e-04\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0412 - val_loss: 8.3953e-04\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0321 - val_loss: 0.0011\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0438 - val_loss: 9.4886e-04\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0412 - val_loss: 7.1983e-04\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0342 - val_loss: 5.7568e-04\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0338 - val_loss: 5.4438e-04\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0296 - val_loss: 5.4975e-04\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0300 - val_loss: 5.9499e-04\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0365 - val_loss: 6.4862e-04\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0301 - val_loss: 6.8838e-04\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0310 - val_loss: 6.6347e-04\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0344 - val_loss: 7.2156e-04\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0282 - val_loss: 8.2525e-04\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0247 - val_loss: 8.9548e-04\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.0497 - val_loss: 7.8331e-04\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.0265 - val_loss: 7.4953e-04\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0374 - val_loss: 9.4448e-04\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.0202 - val_loss: 0.0013\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0309 - val_loss: 0.0017\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0203 - val_loss: 0.0020\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0226 - val_loss: 0.0019\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0319 - val_loss: 0.0015\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0196 - val_loss: 0.0012\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0339 - val_loss: 0.0011\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0201 - val_loss: 0.0010\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0327 - val_loss: 0.0011\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0258 - val_loss: 0.0015\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.0227 - val_loss: 0.0020\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0265 - val_loss: 0.0026\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0251 - val_loss: 0.0023\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0254 - val_loss: 0.0019\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0352 - val_loss: 0.0019\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0260 - val_loss: 0.0021\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.0248 - val_loss: 0.0026\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.0304 - val_loss: 0.0030\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.0200 - val_loss: 0.0028\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0255 - val_loss: 0.0029\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0298 - val_loss: 0.0031\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0235 - val_loss: 0.0030\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.0199 - val_loss: 0.0021\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0308 - val_loss: 0.0015\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0278 - val_loss: 0.0011\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0169 - val_loss: 9.5445e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd7f46a1320>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Reshape\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM, LeakyReLU\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import h5py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:\n",
    "    datas = hf['inputs'].value\n",
    "    labels = hf['outputs'].value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "step_size = datas.shape[1]\n",
    "units= 50\n",
    "second_units = 30\n",
    "batch_size = 8\n",
    "nb_features = datas.shape[2]\n",
    "epochs = 100\n",
    "output_size=16\n",
    "#split training validation\n",
    "training_size = int(0.8* datas.shape[0])\n",
    "training_datas = datas[:training_size,:]\n",
    "training_labels = labels[:training_size,:,0]\n",
    "validation_datas = datas[training_size:,:]\n",
    "validation_labels = labels[training_size:,:,0]\n",
    "\n",
    "\n",
    "#build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=units,activation='tanh', input_shape=(step_size,nb_features),return_sequences=False))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(output_size))\n",
    "model.add(LeakyReLU())\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(training_datas, training_labels, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17281975, 0.16930988, 0.18131165, 0.1885862 , 0.15549396,\n",
       "        0.16543642, 0.17254569, 0.18146437, 0.16966446, 0.16211414,\n",
       "        0.19566512, 0.1770158 , 0.18560435, 0.18399076, 0.19063601,\n",
       "        0.22183488],\n",
       "       [0.13400082, 0.127956  , 0.14055097, 0.15238659, 0.11253364,\n",
       "        0.12428259, 0.13279374, 0.14282727, 0.13458249, 0.1215101 ,\n",
       "        0.1618751 , 0.13869512, 0.15199563, 0.14751366, 0.1554972 ,\n",
       "        0.19622879],\n",
       "       [0.14686441, 0.14172186, 0.1539762 , 0.16472931, 0.12672871,\n",
       "        0.13748255, 0.14620835, 0.1558702 , 0.14635432, 0.13517892,\n",
       "        0.17285267, 0.1511579 , 0.16347384, 0.15991125, 0.16732684,\n",
       "        0.2046789 ],\n",
       "       [0.15890853, 0.15470517, 0.16647899, 0.17664322, 0.13997039,\n",
       "        0.1493381 , 0.1590403 , 0.16832055, 0.15757811, 0.14824653,\n",
       "        0.18289381, 0.16252826, 0.17463998, 0.17189479, 0.17860083,\n",
       "        0.21260113],\n",
       "       [0.14076911, 0.13498537, 0.14771497, 0.15815452, 0.12008433,\n",
       "        0.13232093, 0.13923952, 0.14927016, 0.14027452, 0.12813377,\n",
       "        0.16815934, 0.1459388 , 0.15712678, 0.15321165, 0.16144368,\n",
       "        0.20074475],\n",
       "       [0.16844586, 0.1650222 , 0.1763087 , 0.18601915, 0.15057984,\n",
       "        0.15904336, 0.16925655, 0.17832266, 0.16641274, 0.15849796,\n",
       "        0.19093543, 0.17184266, 0.18304233, 0.18142557, 0.18784693,\n",
       "        0.2191545 ]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(validation_datas)\n",
    "predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lauzhack2",
   "language": "python",
   "name": "lauzhack2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
